{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import operator\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gets names of all attributes from spambase.names file\n",
    "ma = open(\"spambase.names\",\"r\").read().split(\"\\n\")[-58:]\n",
    "ma = [x.split(\":\")[0] for x in ma]\n",
    "#rename last column to class\n",
    "ma[-1] = \"class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extracts data from spambase.data\n",
    "mails = pd.read_csv(\"spambase.data\",names = ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...    char_freq_;  char_freq_(  \\\n",
       "0             0.00            0.00  ...           0.00        0.000   \n",
       "1             0.00            0.94  ...           0.00        0.132   \n",
       "2             0.64            0.25  ...           0.01        0.143   \n",
       "3             0.31            0.63  ...           0.00        0.137   \n",
       "4             0.31            0.63  ...           0.00        0.135   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0          0.0        0.778        0.000        0.000   \n",
       "1          0.0        0.372        0.180        0.048   \n",
       "2          0.0        0.276        0.184        0.010   \n",
       "3          0.0        0.137        0.000        0.000   \n",
       "4          0.0        0.135        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  class  \n",
       "0                       278      1  \n",
       "1                      1028      1  \n",
       "2                      2259      1  \n",
       "3                       191      1  \n",
       "4                       191      1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Attributes: 58 (57 continuous, 1 nominal class label)\n",
    "# spam (1) or not (0)\n",
    "mails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAADuCAYAAAAOR30qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wKhN3pz5kD/SgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2306247cc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#show percentage of spam and ham\n",
    "ham_no,spam_no=mails[\"class\"].value_counts()\n",
    "plt.pie((ham_no,spam_no),labels=(\"ham\",\"spam\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.213015</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.065425</td>\n",
       "      <td>0.312223</td>\n",
       "      <td>0.095901</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>5.191515</td>\n",
       "      <td>52.172789</td>\n",
       "      <td>283.289285</td>\n",
       "      <td>0.394045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.305358</td>\n",
       "      <td>1.290575</td>\n",
       "      <td>0.504143</td>\n",
       "      <td>1.395151</td>\n",
       "      <td>0.672513</td>\n",
       "      <td>0.273824</td>\n",
       "      <td>0.391441</td>\n",
       "      <td>0.401071</td>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.644755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.109394</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.429342</td>\n",
       "      <td>31.729449</td>\n",
       "      <td>194.891310</td>\n",
       "      <td>606.347851</td>\n",
       "      <td>0.488698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.276000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.706000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.540000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>42.810000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.385000</td>\n",
       "      <td>9.752000</td>\n",
       "      <td>4.081000</td>\n",
       "      <td>32.478000</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.829000</td>\n",
       "      <td>1102.500000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>15841.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "count     4601.000000        4601.000000    4601.000000   4601.000000   \n",
       "mean         0.104553           0.213015       0.280656      0.065425   \n",
       "std          0.305358           1.290575       0.504143      1.395151   \n",
       "min          0.000000           0.000000       0.000000      0.000000   \n",
       "25%          0.000000           0.000000       0.000000      0.000000   \n",
       "50%          0.000000           0.000000       0.000000      0.000000   \n",
       "75%          0.000000           0.000000       0.420000      0.000000   \n",
       "max          4.540000          14.280000       5.100000     42.810000   \n",
       "\n",
       "       word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "count    4601.000000     4601.000000       4601.000000         4601.000000   \n",
       "mean        0.312223        0.095901          0.114208            0.105295   \n",
       "std         0.672513        0.273824          0.391441            0.401071   \n",
       "min         0.000000        0.000000          0.000000            0.000000   \n",
       "25%         0.000000        0.000000          0.000000            0.000000   \n",
       "50%         0.000000        0.000000          0.000000            0.000000   \n",
       "75%         0.380000        0.000000          0.000000            0.000000   \n",
       "max        10.000000        5.880000          7.270000           11.110000   \n",
       "\n",
       "       word_freq_order  word_freq_mail     ...       char_freq_;  char_freq_(  \\\n",
       "count      4601.000000     4601.000000     ...       4601.000000  4601.000000   \n",
       "mean          0.090067        0.239413     ...          0.038575     0.139030   \n",
       "std           0.278616        0.644755     ...          0.243471     0.270355   \n",
       "min           0.000000        0.000000     ...          0.000000     0.000000   \n",
       "25%           0.000000        0.000000     ...          0.000000     0.000000   \n",
       "50%           0.000000        0.000000     ...          0.000000     0.065000   \n",
       "75%           0.000000        0.160000     ...          0.000000     0.188000   \n",
       "max           5.260000       18.180000     ...          4.385000     9.752000   \n",
       "\n",
       "       char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.016976     0.269071     0.075811     0.044238   \n",
       "std       0.109394     0.815672     0.245882     0.429342   \n",
       "min       0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.315000     0.052000     0.000000   \n",
       "max       4.081000    32.478000     6.003000    19.829000   \n",
       "\n",
       "       capital_run_length_average  capital_run_length_longest  \\\n",
       "count                 4601.000000                 4601.000000   \n",
       "mean                     5.191515                   52.172789   \n",
       "std                     31.729449                  194.891310   \n",
       "min                      1.000000                    1.000000   \n",
       "25%                      1.588000                    6.000000   \n",
       "50%                      2.276000                   15.000000   \n",
       "75%                      3.706000                   43.000000   \n",
       "max                   1102.500000                 9989.000000   \n",
       "\n",
       "       capital_run_length_total        class  \n",
       "count               4601.000000  4601.000000  \n",
       "mean                 283.289285     0.394045  \n",
       "std                  606.347851     0.488698  \n",
       "min                    1.000000     0.000000  \n",
       "25%                   35.000000     0.000000  \n",
       "50%                   95.000000     0.000000  \n",
       "75%                  266.000000     1.000000  \n",
       "max                15841.000000     1.000000  \n",
       "\n",
       "[8 rows x 58 columns]"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate descriptive statistics that summarize the central tendency, dispersion and shape of a dataset’s distribution,\n",
    "#excluding NaN values.\n",
    "mails.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>word_freq_conference</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail            ...             \\\n",
       "0             0.00            0.00            ...              \n",
       "1             0.00            0.94            ...              \n",
       "2             0.64            0.25            ...              \n",
       "3             0.31            0.63            ...              \n",
       "4             0.31            0.63            ...              \n",
       "\n",
       "   word_freq_conference  char_freq_;  char_freq_(  char_freq_[  char_freq_!  \\\n",
       "0                   0.0         0.00        0.000          0.0        0.778   \n",
       "1                   0.0         0.00        0.132          0.0        0.372   \n",
       "2                   0.0         0.01        0.143          0.0        0.276   \n",
       "3                   0.0         0.00        0.137          0.0        0.137   \n",
       "4                   0.0         0.00        0.135          0.0        0.135   \n",
       "\n",
       "   char_freq_$  char_freq_#  capital_run_length_average  \\\n",
       "0        0.000        0.000                       3.756   \n",
       "1        0.180        0.048                       5.114   \n",
       "2        0.184        0.010                       9.821   \n",
       "3        0.000        0.000                       3.537   \n",
       "4        0.000        0.000                       3.537   \n",
       "\n",
       "   capital_run_length_longest  capital_run_length_total  \n",
       "0                          61                       278  \n",
       "1                         101                      1028  \n",
       "2                         485                      2259  \n",
       "3                          40                       191  \n",
       "4                          40                       191  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels are the values containing whether its a spam or not\n",
    "labels = np.array(mails['class'])\n",
    "\n",
    "# Remove the class labels from the features\n",
    "# axis 1 refers to the columns\n",
    "features= mails.drop('class', axis = 1)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving feature names for later use(except class)\n",
    "feature_list = ma[:-1]\n",
    "\n",
    "# Convert to numpy array\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (3067, 57)\n",
      "Training Labels Shape: (3067,)\n",
      "Testing Features Shape: (1534, 57)\n",
      "Testing Labels Shape: (1534,)\n",
      "\n",
      "Accuracy for 1 th fold is: 0.95 \n",
      "\n",
      "   false positive  false negative  overall error\n",
      "0              22              52           4.82 \n",
      "\n",
      "\n",
      "\n",
      "Training Features Shape: (3067, 57)\n",
      "Training Labels Shape: (3067,)\n",
      "Testing Features Shape: (1534, 57)\n",
      "Testing Labels Shape: (1534,)\n",
      "\n",
      "Accuracy for 2 th fold is: 0.95 \n",
      "\n",
      "   false positive  false negative  overall error\n",
      "0              28              43           4.63 \n",
      "\n",
      "\n",
      "\n",
      "Training Features Shape: (3068, 57)\n",
      "Training Labels Shape: (3068,)\n",
      "Testing Features Shape: (1533, 57)\n",
      "Testing Labels Shape: (1533,)\n",
      "\n",
      "Accuracy for 3 th fold is: 0.95 \n",
      "\n",
      "   false positive  false negative  overall error\n",
      "0              34              45           5.15 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initialise overall error rate \n",
    "oer = 0\n",
    "#count fold number\n",
    "k=0\n",
    "#create fold, 3 times , shuffle=True will shuffle the data before splitting into batches.\n",
    "kf = KFold(n_splits=3,shuffle=True)\n",
    "for train_index, test_index in kf.split(features):\n",
    "    k+=1\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    #split the data into training and testing\n",
    "    train_features, test_features = features[train_index], features[test_index]\n",
    "    train_labels, test_labels = labels[train_index], labels[test_index]\n",
    "    print('Training Features Shape:', train_features.shape)\n",
    "    print('Training Labels Shape:', train_labels.shape)\n",
    "    print('Testing Features Shape:', test_features.shape)\n",
    "    print('Testing Labels Shape:', test_labels.shape)\n",
    "\n",
    "    #print(train_labels)\n",
    "    #print(test_labels)\n",
    "    #apply random forest classifier, where random state is the seed used by the random number generator\n",
    "    rf = RandomForestClassifier(n_estimators = 500, random_state = 42)\n",
    "    #train the model on given data\n",
    "    rf.fit(train_features, train_labels)\n",
    "    \n",
    "    # Use the forest's predict method on the test data\n",
    "    predictions = rf.predict(test_features)\n",
    "    #get accuracy by comparing test label and predictions\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    # Calculate the absolute errors\n",
    "    errors = abs(predictions - test_labels)\n",
    "    # Calculate mean absolute percentage error (MAPE)\n",
    "    mean_error = 100 * (sum(errors) / len(test_labels))\n",
    "    print('\\nAccuracy for',k,\"th fold is:\", round(accuracy, 2),\"\\n\")\n",
    "    #generate confusion matrix which has false positive, flase negative, true positive and true negative\n",
    "    cm = confusion_matrix(test_labels,predictions , labels=None)\n",
    "    #extract false positive and false negative\n",
    "    fp = cm[0,1]\n",
    "    fn = cm[1,0]\n",
    "    mean_error = round(mean_error, 2)\n",
    "    name = np.array([\"false positive\",\"false negative\",\"overall error\"])\n",
    "    data = [[fp,fn,mean_error]]\n",
    "    table_rf.append(data)\n",
    "    observation = pd.DataFrame(data,columns = name)\n",
    "    #add the mean error to calculate average error rate accross all folds\n",
    "    oer += mean_error\n",
    "    print(observation,\"\\n\\n\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean error( over all folds) rate is : 4.87\n",
      "\n",
      "In last k fold:\n",
      "\n",
      "Mislabeled: 79\n",
      "\n",
      "Most important 5 features are:\n",
      "\n",
      " char_freq_!                   0.125880\n",
      "char_freq_$                   0.095534\n",
      "word_freq_remove              0.075073\n",
      "capital_run_length_average    0.065065\n",
      "word_freq_your                0.064257\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAD8CAYAAACM7CYUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njv1/wRYpCx7xWlIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2305d78a8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"mean error( over all folds) rate is :\",round(oer/3,2))\n",
    "\n",
    "feature_imp = pd.Series(rf.feature_importances_,index=feature_list).sort_values(ascending=False)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('\\nIn last k fold:\\n\\nMislabeled:', sum(errors))\n",
    "print(\"\\nMost important 5 features are:\\n\\n\",feature_imp[:5])\n",
    "#display most important features\n",
    "ind = np.arange(5)\n",
    "fig, ax = plt.subplots()    \n",
    "width = 0.75 # the width of the bars \n",
    "ind = np.arange(5) \n",
    "ax.barh(ind, feature_imp[:5], width, color=\"blue\")\n",
    "ax.set_yticks(ind+width/2)\n",
    "ax.set_yticklabels(feature_imp.index[:5], minor=False)      \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (3067, 57)\n",
      "Training Labels Shape: (3067,)\n",
      "Testing Features Shape: (1534, 57)\n",
      "Testing Labels Shape: (1534,)\n",
      "\n",
      "Accuracy for 1 th fold is: 0.92 \n",
      "\n",
      "   false positive  false negative  overall error\n",
      "0              54              65           7.76 \n",
      "\n",
      "\n",
      "\n",
      "Training Features Shape: (3067, 57)\n",
      "Training Labels Shape: (3067,)\n",
      "Testing Features Shape: (1534, 57)\n",
      "Testing Labels Shape: (1534,)\n",
      "\n",
      "Accuracy for 2 th fold is: 0.93 \n",
      "\n",
      "   false positive  false negative  overall error\n",
      "0              37              73           7.17 \n",
      "\n",
      "\n",
      "\n",
      "Training Features Shape: (3068, 57)\n",
      "Training Labels Shape: (3068,)\n",
      "Testing Features Shape: (1533, 57)\n",
      "Testing Labels Shape: (1533,)\n",
      "\n",
      "Accuracy for 3 th fold is: 0.93 \n",
      "\n",
      "   false positive  false negative  overall error\n",
      "0              45              62           6.98 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oer = 0\n",
    "k=0\n",
    "kf = KFold(n_splits=3,shuffle=True)\n",
    "for train_index, test_index in kf.split(features):\n",
    "    k+=1\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    train_features, test_features = features[train_index], features[test_index]\n",
    "    train_labels, test_labels = labels[train_index], labels[test_index]\n",
    "    print('Training Features Shape:', train_features.shape)\n",
    "    print('Training Labels Shape:', train_labels.shape)\n",
    "    print('Testing Features Shape:', test_features.shape)\n",
    "    print('Testing Labels Shape:', test_labels.shape)\n",
    "\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # Use the logistic regressions predict method on the test data\n",
    "    predictions = classifier.predict(test_features)\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    # Calculate the absolute errors\n",
    "    errors = abs(predictions - test_labels)\n",
    "    # Calculate mean absolute percentage error (MAPE)\n",
    "    mean_error = 100 * (sum(errors) / len(test_labels))\n",
    "    print('\\nAccuracy for',k,\"th fold is:\", round(accuracy, 2),\"\\n\")\n",
    "    cm = confusion_matrix(test_labels,predictions , labels=None)\n",
    "    #extract false positive and false negative\n",
    "    fp = cm[0,1]\n",
    "    fn = cm[1,0]\n",
    "    mean_error = round(mean_error, 2)\n",
    "    name = np.array([\"false positive\",\"false negative\",\"overall error\"])\n",
    "    data = [[fp,fn,mean_error]]\n",
    "    observation = pd.DataFrame(data,columns = name)\n",
    "    oer += mean_error\n",
    "    print(observation,\"\\n\\n\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean error( over all folds) rate is : 7.3\n",
      "\n",
      "In last k fold Mislabeled: 107\n"
     ]
    }
   ],
   "source": [
    "print(\"mean error( over all folds) rate is :\",round(oer/3,2))\n",
    "print('\\nIn last k fold Mislabeled:', sum(errors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (3067, 57)\n",
      "Training Labels Shape: (3067,)\n",
      "Testing Features Shape: (1534, 57)\n",
      "Testing Labels Shape: (1534,)\n",
      "\n",
      "Accuracy for 1 th fold is: 0.78 \n",
      "\n",
      "   false positive  false negative  overall error\n",
      "0             150             192          22.29 \n",
      "\n",
      "\n",
      "\n",
      "Training Features Shape: (3067, 57)\n",
      "Training Labels Shape: (3067,)\n",
      "Testing Features Shape: (1534, 57)\n",
      "Testing Labels Shape: (1534,)\n",
      "\n",
      "Accuracy for 2 th fold is: 0.79 \n",
      "\n",
      "   false positive  false negative  overall error\n",
      "0             153             168          20.93 \n",
      "\n",
      "\n",
      "\n",
      "Training Features Shape: (3068, 57)\n",
      "Training Labels Shape: (3068,)\n",
      "Testing Features Shape: (1533, 57)\n",
      "Testing Labels Shape: (1533,)\n",
      "\n",
      "Accuracy for 3 th fold is: 0.81 \n",
      "\n",
      "   false positive  false negative  overall error\n",
      "0             152             145          19.37 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=3,shuffle=True)\n",
    "oer=0\n",
    "k=0\n",
    "for train_index, test_index in kf.split(features):\n",
    "    k+=1\n",
    "    train_features, test_features = features[train_index], features[test_index]\n",
    "    train_labels, test_labels = labels[train_index], labels[test_index]\n",
    "    print('Training Features Shape:', train_features.shape)\n",
    "    print('Training Labels Shape:', train_labels.shape)\n",
    "    print('Testing Features Shape:', test_features.shape)\n",
    "    print('Testing Labels Shape:', test_labels.shape)\n",
    "\n",
    "    #print(train_labels)\n",
    "    #print(test_labels)\n",
    "    mnb = MultinomialNB().fit(train_features, train_labels)\n",
    "    # Use the Naive bayes's predict method on the test data\n",
    "    predictions = mnb.predict(test_features)\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    # Calculate the absolute errors\n",
    "    errors = abs(predictions - test_labels)\n",
    "    # Calculate mean absolute percentage error (MAPE)\n",
    "    mean_error = 100 * (sum(errors) / len(test_labels))\n",
    "    print('\\nAccuracy for',k,\"th fold is:\", round(accuracy, 2),\"\\n\")\n",
    "    cm = confusion_matrix(test_labels,predictions , labels=None)\n",
    "    #extract false positive and false negative\n",
    "    fp = cm[0,1]\n",
    "    fn = cm[1,0]\n",
    "    mean_error = round(mean_error, 2)\n",
    "    name = np.array([\"false positive\",\"false negative\",\"overall error\"])\n",
    "    data = [[fp,fn,mean_error]]\n",
    "    observation = pd.DataFrame(data,columns = name)\n",
    "    oer += mean_error\n",
    "    print(observation,\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean error( over all folds) rate is : 20.86\n",
      "\n",
      "In last k fold Mislabeled: 297\n"
     ]
    }
   ],
   "source": [
    "print(\"mean error( over all folds) rate is :\",round(oer/3,2))\n",
    "\n",
    "feature_imp = pd.Series(rf.feature_importances_,index=feature_list).sort_values(ascending=False)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('\\nIn last k fold Mislabeled:', sum(errors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (3067, 57)\n",
      "Training Labels Shape: (3067,)\n",
      "Testing Features Shape: (1534, 57)\n",
      "Testing Labels Shape: (1534,)\n",
      "\n",
      "Accuracy for 1 th fold is: 0.86 \n",
      "\n",
      "   false positive  false negative  overall error\n",
      "0             162              52          13.95 \n",
      "\n",
      "\n",
      "\n",
      "Training Features Shape: (3067, 57)\n",
      "Training Labels Shape: (3067,)\n",
      "Testing Features Shape: (1534, 57)\n",
      "Testing Labels Shape: (1534,)\n",
      "\n",
      "Accuracy for 2 th fold is: 0.89 \n",
      "\n",
      "   false positive  false negative  overall error\n",
      "0              80              85          10.76 \n",
      "\n",
      "\n",
      "\n",
      "Training Features Shape: (3068, 57)\n",
      "Training Labels Shape: (3068,)\n",
      "Testing Features Shape: (1533, 57)\n",
      "Testing Labels Shape: (1533,)\n",
      "\n",
      "Accuracy for 3 th fold is: 0.84 \n",
      "\n",
      "   false positive  false negative  overall error\n",
      "0             215              33          16.18 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=3,shuffle=True)\n",
    "k=0\n",
    "oer=0\n",
    "for train_index, test_index in kf.split(features):\n",
    "    k+=1\n",
    "    train_features, test_features = features[train_index], features[test_index]\n",
    "    train_labels, test_labels = labels[train_index], labels[test_index]\n",
    "    print('Training Features Shape:', train_features.shape)\n",
    "    print('Training Labels Shape:', train_labels.shape)\n",
    "    print('Testing Features Shape:', test_features.shape)\n",
    "    print('Testing Labels Shape:', test_labels.shape)\n",
    "\n",
    "    #print(train_labels)\n",
    "    #print(test_labels)\n",
    "    #Create SVM model with l2 normalisation, The “balanced” mode uses the values of y to automatically adjust weights \n",
    "    #inversely proportional to class frequencies in the input data\n",
    "    svm = LinearSVC(penalty=\"l2\", loss=\"squared_hinge\",class_weight=\"balanced\", max_iter = 2000)\n",
    "    svm.fit(train_features, train_labels) \n",
    "    predictions = svm.predict(test_features)\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    # Calculate the absolute errors\n",
    "    errors = abs(predictions - test_labels)\n",
    "    # Calculate mean absolute percentage error (MAPE)\n",
    "    mean_error = 100 * (sum(errors) / len(test_labels))\n",
    "    print('\\nAccuracy for',k,\"th fold is:\", round(accuracy, 2),\"\\n\")\n",
    "    cm = confusion_matrix(test_labels,predictions , labels=None)\n",
    "    fp = cm[0,1]\n",
    "    fn = cm[1,0]\n",
    "    mean_error = round(mean_error, 2)\n",
    "    name = np.array([\"false positive\",\"false negative\",\"overall error\"])\n",
    "    data = [[fp,fn,mean_error]]\n",
    "    observation = pd.DataFrame(data,columns = name)\n",
    "    oer += mean_error\n",
    "    print(observation,\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean error( over all folds) rate is : 13.63\n",
      "\n",
      "In last k fold Mislabeled: 248\n"
     ]
    }
   ],
   "source": [
    "print(\"mean error( over all folds) rate is :\",round(oer/3,2))\n",
    "\n",
    "# Print out the mean absolute error (mae)\n",
    "print('\\nIn last k fold Mislabeled:', sum(errors))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
